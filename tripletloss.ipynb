{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Noel C. F. Codella\n",
    "# Example Triplet Loss Code for Keras / TensorFlow\n",
    "\n",
    "# Implementing Improved Triplet Loss from:\n",
    "# Zhang et al. \"Tracking Persons-of-Interest via Adaptive Discriminative Features\" ECCV 2016\n",
    "\n",
    "# Got help from multiple web sources, including:\n",
    "# 1) https://stackoverflow.com/questions/47727679/triplet-model-for-image-retrieval-from-the-keras-pretrained-network\n",
    "# 2) https://ksaluja15.github.io/Learning-Rate-Multipliers-in-Keras/\n",
    "# 3) https://keras.io/preprocessing/image/\n",
    "# 4) https://github.com/keras-team/keras/issues/3386\n",
    "# 5) https://github.com/keras-team/keras/issues/8130\n",
    "\n",
    "\n",
    "# GLOBAL DEFINES\n",
    "T_G_WIDTH = 224\n",
    "T_G_HEIGHT = 224\n",
    "T_G_NUMCHANNELS = 3\n",
    "T_G_SEED = 1337\n",
    "\n",
    "# Misc. Necessities\n",
    "import sys\n",
    "import ssl # these two lines solved issues loading pretrained model\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "#from scipy.misc import imresize\n",
    "np.random.seed(T_G_SEED)\n",
    "\n",
    "# TensorFlow Includes\n",
    "import tensorflow as tf\n",
    "#from tensorflow.contrib.losses import metric_learning\n",
    "tf.random.set_seed(T_G_SEED)\n",
    "\n",
    "# Keras Imports & Defines \n",
    "import keras\n",
    "import keras.applications\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import keras.layers as kl\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Generator object for data augmentation.\n",
    "# Can change values here to affect augmentation style.\n",
    "datagen = ImageDataGenerator(  rotation_range=90,\n",
    "                                width_shift_range=0.05,\n",
    "                                height_shift_range=0.05,\n",
    "                                zoom_range=0.1,\n",
    "                                horizontal_flip=True,\n",
    "                                vertical_flip=True,\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.legacy import interfaces\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Optimizer\n",
    "\n",
    "class LR_SGD(Optimizer):\n",
    "    \"\"\"Stochastic gradient descent optimizer.\n",
    "\n",
    "    Includes support for momentum,\n",
    "    learning rate decay, and Nesterov momentum.\n",
    "\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        momentum: float >= 0. Parameter updates momentum.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        nesterov: boolean. Whether to apply Nesterov momentum.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, momentum=0., decay=0.,\n",
    "                 nesterov=False,multipliers=None,**kwargs):\n",
    "        super(LR_SGD, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.learning_rate = K.variable(learning_rate, name='learning_rate')\n",
    "            self.momentum = K.variable(momentum, name='momentum')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "        self.initial_decay = decay\n",
    "        self.nesterov = nesterov\n",
    "        self.lr_multipliers = multipliers\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        learning_rate = self.learning_rate\n",
    "        if self.initial_decay > 0:\n",
    "            learning_rate *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "        # momentum\n",
    "        shapes = [K.int_shape(p) for p in params]\n",
    "        moments = [K.zeros(shape) for shape in shapes]\n",
    "        self.weights = [self.iterations] + moments\n",
    "        for p, g, m in zip(params, grads, moments):\n",
    "            \n",
    "            matched_layer = [x for x in self.lr_multipliers.keys() if x in p.name]\n",
    "            if matched_layer:\n",
    "                new_lr = learning_rate * self.lr_multipliers[matched_layer[0]]\n",
    "            else:\n",
    "                new_lr = learning_rate\n",
    "\n",
    "            v = self.momentum * m - new_lr * g  # velocity\n",
    "            self.updates.append(K.update(m, v))\n",
    "\n",
    "            if self.nesterov:\n",
    "                new_p = p + self.momentum * v - new_lr * g\n",
    "            else:\n",
    "                new_p = p + v\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'learning_rate': float(K.get_value(self.learning_rate)),\n",
    "                  'momentum': float(K.get_value(self.momentum)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'nesterov': self.nesterov}\n",
    "        base_config = super(LR_SGD, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.legacy import interfaces\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Optimizer\n",
    "\n",
    "class Adam_lr_mult(Optimizer):\n",
    "    \"\"\"Adam optimizer.\n",
    "    Adam optimizer, with learning rate multipliers built on Keras implementation\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
    "            algorithm from the paper \"On the Convergence of Adam and\n",
    "            Beyond\".\n",
    "    # References\n",
    "        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n",
    "        - [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n",
    "        \n",
    "    AUTHOR: Erik Brorson\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., amsgrad=False,\n",
    "                 multipliers=None, debug_verbose=False,**kwargs):\n",
    "        super(Adam_lr_mult, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsgrad = amsgrad\n",
    "        self.multipliers = multipliers\n",
    "        self.debug_verbose = debug_verbose\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                  K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "\n",
    "            # Learning rate multipliers\n",
    "            if self.multipliers:\n",
    "                multiplier = [mult for mult in self.multipliers if mult in p.name]\n",
    "            else:\n",
    "                multiplier = None\n",
    "            if multiplier:\n",
    "                new_lr_t = lr_t * self.multipliers[multiplier[0]]\n",
    "                if self.debug_verbose:\n",
    "                    print('Setting {} to learning rate {}'.format(multiplier[0], new_lr_t))\n",
    "                    print(K.get_value(new_lr_t))\n",
    "            else:\n",
    "                new_lr_t = lr_t\n",
    "                if self.debug_verbose:\n",
    "                    print('No change in learning rate {}'.format(p.name))\n",
    "                    print(K.get_value(new_lr_t))\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                p_t = p - new_lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                p_t = p - new_lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'amsgrad': self.amsgrad,\n",
    "                  'multipliers':self.multipliers}\n",
    "        base_config = super(Adam_lr_mult, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Local Imports\n",
    "#from LR_SGD import LR_SGD\n",
    "\n",
    "# generator function for data augmentation\n",
    "def createDataGen(X1, X2, X3, Y, b):\n",
    "\n",
    "    local_seed = T_G_SEED\n",
    "    genX1 = datagen.flow(X1,Y, batch_size=b, seed=local_seed, shuffle=False)\n",
    "    genX2 = datagen.flow(X2,Y, batch_size=b, seed=local_seed, shuffle=False)\n",
    "    genX3 = datagen.flow(X3,Y, batch_size=b, seed=local_seed, shuffle=False)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            X2i = genX2.next()\n",
    "            X3i = genX3.next()\n",
    "\n",
    "            yield [X1i[0], X2i[0], X3i[0]], X1i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def createModel(emb_size):\n",
    "\n",
    "    # Initialize a ResNet50_ImageNet Model\n",
    "    resnet_input = kl.Input(shape=(T_G_WIDTH,T_G_HEIGHT,T_G_NUMCHANNELS))\n",
    "    resnet_model = keras.applications.resnet50.ResNet50(weights='imagenet', include_top = False, input_tensor=resnet_input)\n",
    "\n",
    "    # New Layers over ResNet50\n",
    "    net = resnet_model.output\n",
    "    #net = kl.Flatten(name='flatten')(net)\n",
    "    net = kl.GlobalAveragePooling2D(name='gap')(net)\n",
    "    #net = kl.Dropout(0.5)(net)\n",
    "    net = kl.Dense(emb_size,activation='relu',name='t_emb_1')(net)\n",
    "    net = kl.Lambda(lambda  x: K.l2_normalize(x,axis=1), name='t_emb_1_l2norm')(net)\n",
    "\n",
    "    # model creation\n",
    "    base_model = Model(resnet_model.input, net, name=\"base_model\")\n",
    "\n",
    "    # triplet framework, shared weights\n",
    "    input_shape=(T_G_WIDTH,T_G_HEIGHT,T_G_NUMCHANNELS)\n",
    "    input_anchor = kl.Input(shape=input_shape, name='input_anchor')\n",
    "    input_positive = kl.Input(shape=input_shape, name='input_pos')\n",
    "    input_negative = kl.Input(shape=input_shape, name='input_neg')\n",
    "\n",
    "    net_anchor = base_model(input_anchor)\n",
    "    net_positive = base_model(input_positive)\n",
    "    net_negative = base_model(input_negative)\n",
    "\n",
    "    # The Lamda layer produces output using given function. Here its Euclidean distance.\n",
    "    positive_dist = kl.Lambda(euclidean_distance, name='pos_dist')([net_anchor, net_positive])\n",
    "    negative_dist = kl.Lambda(euclidean_distance, name='neg_dist')([net_anchor, net_negative])\n",
    "    tertiary_dist = kl.Lambda(euclidean_distance, name='ter_dist')([net_positive, net_negative])\n",
    "\n",
    "    # This lambda layer simply stacks outputs so both distances are available to the objective\n",
    "    stacked_dists = kl.Lambda(lambda vects: K.stack(vects, axis=1), name='stacked_dists')([positive_dist, negative_dist, tertiary_dist])\n",
    "\n",
    "    model = Model([input_anchor, input_positive, input_negative], stacked_dists, name='triple_siamese')\n",
    "\n",
    "    # Setting up optimizer designed for variable learning rate\n",
    "\n",
    "    # Variable Learning Rate per Layers\n",
    "    lr_mult_dict = {}\n",
    "    last_layer = ''\n",
    "    for layer in resnet_model.layers:\n",
    "        # comment this out to refine earlier layers\n",
    "        # layer.trainable = False  \n",
    "        # print layer.name\n",
    "        lr_mult_dict[layer.name] = 1\n",
    "        # last_layer = layer.name\n",
    "    lr_mult_dict['t_emb_1'] = 100\n",
    "\n",
    "    base_lr = 0.1\n",
    "    momentum = 0.9\n",
    "    v_optimizer = LR_SGD(learning_rate=base_lr, momentum=momentum, decay=0.0, nesterov=False, multipliers = lr_mult_dict)\n",
    "    #print(lr_mult_dict)\n",
    "    #adam_with_lr_multipliers = Adam_lr_mult(multipliers=lr_mult_dict)\n",
    "    \n",
    "    model.compile(optimizer=v_optimizer, loss=triplet_loss, metrics=[accuracy])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred):\n",
    "    margin = K.constant(1)\n",
    "    return K.mean(K.maximum(K.constant(0), K.square(y_pred[:,0,0]) - 0.5*(K.square(y_pred[:,1,0])+K.square(y_pred[:,2,0])) + margin))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return K.mean(y_pred[:,0,0] < y_pred[:,1,0])\n",
    "\n",
    "def l2Norm(x):\n",
    "    return  K.l2_normalize(x, axis=-1)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loads an image and preprocesses\n",
    "def t_read_image(loc):\n",
    "    t_image = cv2.imread(loc)\n",
    "    t_image = cv2.resize(t_image, (T_G_HEIGHT,T_G_WIDTH))\n",
    "    t_image = t_image.astype(\"float32\")\n",
    "    t_image = keras.applications.resnet50.preprocess_input(t_image, data_format='channels_last')\n",
    "\n",
    "    return t_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loads a set of images from a text index file   \n",
    "def t_read_image_list(flist, start, length):\n",
    "\n",
    "    with open(flist) as f:\n",
    "        content = f.readlines() \n",
    "    #print(flist)\n",
    "    #print(content)\n",
    "    content = [x.strip().split()[0] for x in content] \n",
    "\n",
    "    datalen = length\n",
    "    if (datalen < 0):\n",
    "        datalen = len(content)\n",
    "\n",
    "    if (start + datalen > len(content)):\n",
    "        datalen = len(content) - start\n",
    " \n",
    "    imgset = np.zeros((datalen, T_G_HEIGHT, T_G_WIDTH, T_G_NUMCHANNELS))\n",
    "\n",
    "    for i in range(start, start+datalen):\n",
    "        if ((i-start) < len(content)):\n",
    "            imgset[i-start] = t_read_image(content[i])\n",
    "\n",
    "    return imgset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract(argv):\n",
    "\n",
    "    if len(argv) < 3:\n",
    "        print (\"Usage: \\n\\t <Model Prefix> <Input Image List (TXT)> <Output File (TXT)> \\n\\t\\tExtracts triplet-loss model\")\n",
    "        return\n",
    "\n",
    "    modelpref = argv[0]\n",
    "    imglist = argv[1]\n",
    "    outfile = argv[2]\n",
    "\n",
    "    with open(modelpref + '.json', \"r\") as json_file:\n",
    "        model_json = json_file.read()\n",
    "\n",
    "    loaded_model = keras.models.model_from_json(model_json)\n",
    "    loaded_model.load_weights(modelpref + '.h5')\n",
    "\n",
    "    base_model = loaded_model.get_layer('base_model')\n",
    "\n",
    "    # create a new single input\n",
    "    input_shape=(T_G_WIDTH,T_G_HEIGHT,T_G_NUMCHANNELS)\n",
    "    input_single = kl.Input(shape=input_shape, name='input_single')\n",
    "    \n",
    "    # create a new model without the triple loss\n",
    "    net_single = base_model(input_single)\n",
    "    model = Model(input_single, net_single, name='embedding_net')\n",
    "\n",
    "    chunksize = 1000\n",
    "    total_img = file_numlines(imglist)\n",
    "    total_img_ch = int(np.ceil(total_img / float(chunksize)))\n",
    "\n",
    "    with open(outfile, 'w') as f_handle:\n",
    "\n",
    "        for i in range(0, total_img_ch):\n",
    "            imgs = t_read_image_list(imglist, i*chunksize, chunksize)\n",
    "\n",
    "            vals = model.predict(imgs)\n",
    "    \n",
    "            np.savetxt(f_handle, vals)\n",
    "\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def learn(argv):\n",
    "    \n",
    "    if len(argv) < 10:\n",
    "        print (\"Usage: \\n\\t <Train Anchors (TXT)> <Train Positives (TXT)> <Train Negatives (TXT)> <Val Anchors (TXT)> <Val Positives (TXT)> <Val Negatives (TXT)> <embedding size> <batch size> <num epochs> <output model> \\n\\t\\tLearns triplet-loss model\")\n",
    "        return\n",
    "\n",
    "    in_t_a = argv[0]\n",
    "    in_t_b = argv[1]\n",
    "    in_t_c = argv[2]\n",
    "\n",
    "    in_v_a = argv[3]\n",
    "    in_v_b = argv[4]\n",
    "    in_v_c = argv[5]\n",
    "\n",
    "    emb_size = int(argv[6])\n",
    "    batch = int(argv[7])\n",
    "    numepochs = int(argv[8])\n",
    "    outpath = argv[9] \n",
    "\n",
    "    # chunksize is the number of images we load from disk at a time\n",
    "    chunksize = batch*100\n",
    "    total_t = file_numlines(in_t_a)\n",
    "    total_v = file_numlines(in_v_b)\n",
    "    total_t_ch = int(np.ceil(total_t / float(chunksize)))\n",
    "    total_v_ch = int(np.ceil(total_v / float(chunksize)))\n",
    "\n",
    "    print (\"Dataset has \" + str(total_t) + \" training triplets, and \" + str(total_v) + \" validation triplets.\")\n",
    "\n",
    "    print (\"Creating a model ...\")\n",
    "    model = createModel(emb_size)\n",
    "\n",
    "    print (\"Training loop ...\")\n",
    "    \n",
    "    # manual loop over epochs to support very large sets of triplets\n",
    "    for e in range(0, numepochs):\n",
    "\n",
    "        for t in range(0, total_t_ch):\n",
    "\n",
    "            print ('Epoch ' + str(e) + ': train chunk ' + str(t+1) + '/ ' + str(total_t_ch) + ' ...')\n",
    "\n",
    "            print ('Reading image lists ...')\n",
    "            anchors_t = t_read_image_list(in_t_a, t*chunksize, chunksize)\n",
    "            positives_t = t_read_image_list(in_t_b, t*chunksize, chunksize)\n",
    "            negatives_t = t_read_image_list(in_t_c, t*chunksize, chunksize)\n",
    "            Y_train = np.random.randint(2, size=(1,2,anchors_t.shape[0])).T\n",
    "            \n",
    "            print ('Starting to fit ...')\n",
    "            # This method does NOT use data augmentation\n",
    "            model.fit([anchors_t, positives_t, negatives_t], Y_train, epochs=numepochs,  batch_size=batch)\n",
    "            \n",
    "            # This method uses data augmentation\n",
    "            #model.fit_generator(generator=createDataGen(anchors_t,positives_t,negatives_t,Y_train,batch), steps_per_epoch=len(Y_train) / batch, epochs=1, shuffle=False, use_multiprocessing=True)\n",
    "        \n",
    "        # In case the validation images don't fit in memory, we load chunks from disk again. \n",
    "        val_res = [0.0, 0.0]\n",
    "        total_w = 0.0\n",
    "        for v in range(0, total_v_ch):\n",
    "\n",
    "            print ('Loading validation image lists ...')\n",
    "            print ('Epoch ' + str(e) + ': val chunk ' + str(v+1) + '/ ' + str(total_v_ch) + ' ...')\n",
    "            anchors_v = t_read_image_list(in_v_a, v*chunksize, chunksize)\n",
    "            positives_v = t_read_image_list(in_v_b, v*chunksize, chunksize)\n",
    "            negatives_v = t_read_image_list(in_v_c, v*chunksize, chunksize)\n",
    "            Y_val = np.random.randint(2, size=(1,2,anchors_v.shape[0])).T\n",
    "\n",
    "            \n",
    "            # Weight of current validation measurement. \n",
    "            # if loaded expected number of items, this will be 1.0, otherwise < 1.0, and > 0.0.\n",
    "            w = float(anchors_v.shape[0]) / float(chunksize)\n",
    "            total_w = total_w + w\n",
    "\n",
    "            curval = model.evaluate([anchors_v, positives_v, negatives_v], Y_val, batch_size=batch)\n",
    "            val_res[0] = val_res[0] + w*curval[0]\n",
    "            val_res[1] = val_res[1] + w*curval[1]\n",
    "\n",
    "        val_res = [x / total_w for x in val_res]\n",
    "\n",
    "        print ('Validation Results: ' + str(val_res))\n",
    "\n",
    "    print ('Saving model ...')\n",
    "\n",
    "    # Save the model and weights\n",
    "    model.save(outpath + '.h5')\n",
    "\n",
    "    # Due to some remaining Keras bugs around loading custom optimizers\n",
    "    # and objectives, we save the model architecture as well\n",
    "    model_json = model.to_json()\n",
    "    with open(outpath + '.json', \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 5 training triplets, and 5 validation triplets.\n",
      "Creating a model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/root07/miniconda3/lib/python3.7/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop ...\n",
      "Epoch 0: train chunk 1/ 1 ...\n",
      "Reading image lists ...\n",
      "Starting to fit ...\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "def file_numlines(fn):\n",
    "    with open(fn) as f:\n",
    "        return sum(1 for _ in f)\n",
    "    \n",
    "#def main(argv):\n",
    "argv = [\"learn\", \"anchor\", \"positives\", \"negatives\",\"val_anchor\", \"val_positive\", \"val_negative\", \"128\", \"100\", \"5\", \"ann_trained\"]\n",
    "#if len(argv) < 2:\n",
    "#    print ('Usage: \\n\\t -learn <Train Anchors (TXT)> <Train Positives (TXT)> <Train Negatives (TXT)> <Val Anchors (TXT)> <Val Positives (TXT)> <Val Negatives (TXT)> <embedding size> <batch size> <num epochs> <output model prefix> \\n\\t -extract <Model Prefix> <Input Image List (TXT)> <Output File (TXT)> \\n\\t\\tBuilds and scores a triplet-loss model ')\n",
    "#    return\n",
    "#print(argv[0]+\" \"+argv[9])\n",
    "if 'learn' in argv[0]:\n",
    "    learn(argv[1:])\n",
    "elif 'extract' in argv[0]:\n",
    "    extract(argv[1:])    \n",
    "\n",
    " #   return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
